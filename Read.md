Paraphrase Generation using T5 (Fine-Tuning)
This project demonstrates how to fine-tune the T5 (Text-To-Text Transfer Transformer) model for the task of paraphrase generation. Using a dataset of sentence pairs (original and paraphrased versions), we train T5 to learn how to generate high-quality paraphrases.

ðŸ§  Overview
T5 is a versatile sequence-to-sequence model that treats every NLP task as a text-to-text problem. For paraphrase generation, the model takes an input sentence and is trained to output a paraphrased version of that sentence.

This repository covers:

Data preprocessing and formatting for T5

Fine-tuning the T5 model on paraphrase data

Evaluation and inference

Example scripts and notebook for training and generation
